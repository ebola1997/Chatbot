{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e521d5fb-d1fa-4896-b90d-c6d4a9a52d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15dcdfd5-5c82-4f1b-85a2-6278776346d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca file TXT\n",
    "def read_txt(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Membaca file PDF\n",
    "def read_pdf(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "# Membaca file DOCX\n",
    "def read_docx(filename):\n",
    "    doc = Document(filename)\n",
    "    text = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    return text\n",
    "\n",
    "# Mendapatkan paragraf dari semua file\n",
    "def parse_file(filename):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        content = read_txt(filename)\n",
    "    elif filename.endswith(\".pdf\"):\n",
    "        content = read_pdf(filename)\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        content = read_docx(filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filename}\")\n",
    "    \n",
    "    paragraphs = []\n",
    "    buffer = []\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif len(buffer):\n",
    "            paragraphs.append(\" \".join(buffer))\n",
    "            buffer = []\n",
    "    if len(buffer):\n",
    "        paragraphs.append(\" \".join(buffer))\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12ee8eb-e1d2-4014-9c1e-f2ee07ad8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan paragraf dari semua file\n",
    "def parse_file(filename):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        content = read_txt(filename)\n",
    "    elif filename.endswith(\".pdf\"):\n",
    "        content = read_pdf(filename)\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        content = read_docx(filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filename}\")\n",
    "    \n",
    "    paragraphs = []\n",
    "    buffer = []\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif len(buffer):\n",
    "            paragraphs.append(\" \".join(buffer))\n",
    "            buffer = []\n",
    "    if len(buffer):\n",
    "        paragraphs.append(\" \".join(buffer))\n",
    "    return paragraphs\n",
    "\n",
    "# Simpan embedding ke file\n",
    "def save_embeddings(filename, embeddings):\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    with open(f\"embeddings/{filename}.json\", \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "# Memuat embedding dari file\n",
    "def load_embeddings(filename):\n",
    "    if not os.path.exists(f\"embeddings/{filename}.json\"):\n",
    "        return False\n",
    "    with open(f\"embeddings/{filename}.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Mendapatkan embedding\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    if (embeddings := load_embeddings(filename)) is not False:\n",
    "        return embeddings\n",
    "    embeddings = [\n",
    "        ollama.embeddings(model=modelname, prompt=chunk)[\"embedding\"]\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    save_embeddings(filename, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Cosine similarity untuk menemukan kemiripan\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016466c-83b8-48c4-a5fc-91cc47408156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.post(\"http://localhost:11434/api/chat\", json={\n",
    "                    \"model\": \"llama3\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"nama saya adalah teguh prasetyo umur 25 tahun, pekerjaan dba\" ,\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": \"siapa teguh?\"},\n",
    "                    ]\n",
    "                })\n",
    "    print(response.json())\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbfddf26-5b61-444b-b73e-c7c0bc144a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 200\n",
      "Response Content-Type: application/x-ndjson\n",
      "Response is in NDJSON format.\n",
      "Full Response: Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "1. **Learning**: AI systems can learn from data and improve their performance over time.\n",
      "2. **Reasoning**: AI systems can make decisions based on logic, rules, or patterns.\n",
      "3. **Problem-solving**: AI systems can solve complex problems, like recognizing objects in images or understanding natural language.\n",
      "4. **Perception**: AI systems can perceive their environment through sensors, cameras, or other devices.\n",
      "\n",
      "AI has many applications across various industries, including:\n",
      "\n",
      "1. **Natural Language Processing (NLP)**: Chatbots, virtual assistants, and text analysis.\n",
      "2. **Computer Vision**: Image recognition, object detection, facial recognition, and autonomous vehicles.\n",
      "3. **Robotics**: Industrial robots, service robots, and humanoid robots that can perform tasks autonomously.\n",
      "4. **Predictive Analytics**: Forecasting, recommendation systems, and decision-making support.\n",
      "\n",
      "There are many types of AI, including:\n",
      "\n",
      "1. **Narrow or Weak AI**: Designed to perform a specific task, like playing chess or recognizing faces.\n",
      "2. **General or Strong AI**: Possesses human-like intelligence, capable of reasoning, problem-solving, and learning.\n",
      "3. **Superintelligence**: Far exceeds human intelligence in various domains.\n",
      "\n",
      "The development of AI relies on several key technologies, such as:\n",
      "\n",
      "1. **Machine Learning**: Algorithms that enable AI systems to learn from data without being explicitly programmed.\n",
      "2. **Deep Learning**: A subset of machine learning using neural networks inspired by the human brain.\n",
      "3. **Big Data**: Large datasets that can be used to train and test AI models.\n",
      "\n",
      "AI has many potential benefits, including:\n",
      "\n",
      "1. **Increased Efficiency**: Automating repetitive tasks and freeing up human resources for more strategic work.\n",
      "2. **Improved Accuracy**: Reducing errors and improving decision-making through data-driven insights.\n",
      "3. **Enhanced Customer Experience**: Personalized services and tailored recommendations.\n",
      "\n",
      "However, AI also raises concerns about:\n",
      "\n",
      "1. **Job Displacement**: The potential impact on employment due to automation.\n",
      "2. **Bias and Discrimination**: The risk of perpetuating existing biases in AI systems.\n",
      "3. **Ethical Concerns**: The need for responsible development and deployment of AI technologies.\n",
      "\n",
      "Overall, Artificial Intelligence has the potential to transform many aspects of our lives, from healthcare and finance to education and entertainment. As the field continues to evolve, it's essential to consider both the benefits and challenges AI presents.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # response = requests.post(\"http://localhost:11434/api/generate\", json={\"model\": \"llama3\", \"prompt\": \"what is AI?\"})\n",
    "    \n",
    "    print(\"Response Status Code:\", response.status_code)\n",
    "    print(\"Response Content-Type:\", response.headers.get('Content-Type'))\n",
    "\n",
    "    if response.headers.get('Content-Type') == 'application/x-ndjson':\n",
    "        print(\"Response is in NDJSON format.\")\n",
    "        \n",
    "        full_response = \"\"  # Initialize an empty string to hold the full response\n",
    "        \n",
    "        for line in response.text.splitlines():\n",
    "            try:\n",
    "                # Parse each line as a JSON object\n",
    "                json_data = json.loads(line)\n",
    "                \n",
    "                # Append the 'response' part of each JSON object to the full response\n",
    "                full_response += json_data['response']\n",
    "                \n",
    "                # If the response is complete, print the full response\n",
    "                if json_data.get('done'):\n",
    "                    print(\"Full Response:\", full_response)\n",
    "                    break\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line: {line}\")\n",
    "                print(f\"JSONDecodeError: {e}\")\n",
    "    else:\n",
    "        print(\"Response is not in NDJSON format.\")\n",
    "        print(response.json())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c596731c-4396-4382-a701-9b74620663cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Silakan tanya bosku? (ketik 'exit' untuk keluar) ->  hai bot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Error during request: Extra data: line 2 column 1 (char 120)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Silakan tanya bosku? (ketik 'exit' untuk keluar) ->  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Anda adalah asisten yang membantu menjawab pertanyaan dengan bahasa Indonesia \n",
    "dan berdasarkan cuplikan teks yang diberikan dalam konteks. Jawab hanya menggunakan konteks yang disediakan, \n",
    "menjadi sesingkat mungkin. Jika Anda tidak yakin, katakan saja Anda tidak tahu.\n",
    "Context:\n",
    "\"\"\"\n",
    "\n",
    "data_folder = \"data\"\n",
    "all_paragraphs = []\n",
    "filenames = []\n",
    "\n",
    "# Function to parse files (You need to implement this or use an existing function)\n",
    "def parse_file(file_path):\n",
    "    # Parse the file and return paragraphs (implement your file parsing logic here)\n",
    "    return []\n",
    "\n",
    "# Function to get embeddings (using requests instead of ollama)\n",
    "def get_embeddings(modelname, chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",  # Use the correct endpoint here\n",
    "                json={\"model\": modelname, \"prompt\": chunk}\n",
    "            )\n",
    "            response.raise_for_status()  # Ensure we raise an error for bad status codes\n",
    "            embeddings.append(response.json()[\"embedding\"])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching embeddings: {e}\")\n",
    "            embeddings.append(None)  # Append None or handle this error gracefully\n",
    "    return embeddings\n",
    "\n",
    "# Function to find the most similar chunks (implement your similarity search here)\n",
    "def find_most_similar(prompt_embedding, embeddings):\n",
    "    # Implement your similarity function here\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    global all_paragraphs, filenames\n",
    "\n",
    "    # Iterasi semua file dalam folder data\n",
    "    for file in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        if file.lower().endswith((\".txt\", \".pdf\", \".docx\")):\n",
    "            paragraphs = parse_file(file_path)\n",
    "            all_paragraphs.extend(paragraphs)\n",
    "            filenames.append(file)\n",
    "\n",
    "    # Create embeddings manually using requests\n",
    "    embeddings = get_embeddings(\"nomic-embed-text\", all_paragraphs)\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"Silakan tanya bosku? (ketik 'exit' untuk keluar) -> \")\n",
    "\n",
    "        if prompt.lower() == \"exit\":\n",
    "            print(\"Exiting the assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Manually fetch the prompt embedding using requests\n",
    "            prompt_response = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",  # Use the correct endpoint here\n",
    "                json={\"model\": \"nomic-embed-text\", \"prompt\": prompt}\n",
    "            )\n",
    "            prompt_response.raise_for_status()  # Ensure we raise an error for bad status codes\n",
    "            prompt_embedding = prompt_response.json()[\"embedding\"]\n",
    "            \n",
    "            most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "\n",
    "            # Get the chat response manually\n",
    "            chat_response = requests.post(\n",
    "                \"http://localhost:11434/api/chat\",  # Use the correct endpoint here\n",
    "                json={\n",
    "                    \"model\": \"llama3\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": SYSTEM_PROMPT + \"\\n\".join(all_paragraphs[item[1]] for item in most_similar_chunks),\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            if chat_response.status_code != 200:\n",
    "                print(f\"Error: Received {chat_response.status_code} response\")\n",
    "                print(f\"Response content: {chat_response.text}\")\n",
    "            else:\n",
    "                print(\"\\n\\n\")\n",
    "                print(chat_response.json()[\"message\"][\"content\"])\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error during request: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80dbed1d-4d56-4d28-89d8-a3ba1e46037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# URL untuk API Ollama, ganti sesuai dengan alamat API Anda\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/\"\n",
    "\n",
    "# Fungsi untuk request embeddings\n",
    "def get_embeddings_from_ollama(modelname, chunk):\n",
    "    response = requests.post(\n",
    "        OLLAMA_API_URL + \"embeddings\",\n",
    "        json={\"model\": modelname, \"prompt\": chunk},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"embedding\"]\n",
    "\n",
    "# Fungsi untuk request chat (LLM) response\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Fungsi untuk request chat (LLM) response\n",
    "def chat_with_ollama(model, messages, ollama_api_url):\n",
    "    try:\n",
    "        # Make the POST request to Ollama API's chat endpoint\n",
    "        response = requests.post(\n",
    "            ollama_api_url + \"chat\",\n",
    "            json={\"model\": model, \"messages\": messages},\n",
    "            stream=True  # Use stream=True to handle NDJSON (chunked) responses\n",
    "        )\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        \n",
    "        # Initialize an empty string to hold the complete response\n",
    "        full_response = \"\"\n",
    "        \n",
    "        # Check if the response is in NDJSON format\n",
    "        if response.headers.get('Content-Type') == 'application/x-ndjson':\n",
    "            # Process the NDJSON stream line by line\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line.strip():  # Ignore empty lines\n",
    "                    try:\n",
    "                        # Parse each JSON line from the NDJSON stream\n",
    "                        json_data = json.loads(line)\n",
    "                        \n",
    "                        # Append the part of the response to the full response\n",
    "                        full_response += json_data.get('response', '')\n",
    "                        \n",
    "                        # If the response is done, break the loop\n",
    "                        if json_data.get('done'):\n",
    "                            break\n",
    "                    \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error parsing line: {line}\")\n",
    "                        print(f\"JSONDecodeError: {e}\")\n",
    "        else:\n",
    "            print(f\"Unexpected Content-Type: {response.headers.get('Content-Type')}\")\n",
    "            return None\n",
    "        \n",
    "        return full_response  # Return the complete response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Membaca file TXT\n",
    "def read_txt(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Membaca file PDF\n",
    "def read_pdf(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "# Membaca file DOCX\n",
    "def read_docx(filename):\n",
    "    doc = Document(filename)\n",
    "    text = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    return text\n",
    "\n",
    "# Mendapatkan paragraf dari semua file\n",
    "def parse_file(filename):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        content = read_txt(filename)\n",
    "    elif filename.endswith(\".pdf\"):\n",
    "        content = read_pdf(filename)\n",
    "    elif filename.endswith(\".docx\"):\n",
    "        content = read_docx(filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filename}\")\n",
    "    \n",
    "    paragraphs = []\n",
    "    buffer = []\n",
    "    for line in content.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif len(buffer):\n",
    "            paragraphs.append(\" \".join(buffer))\n",
    "            buffer = []\n",
    "    if len(buffer):\n",
    "        paragraphs.append(\" \".join(buffer))\n",
    "    return paragraphs\n",
    "\n",
    "# Mendapatkan embedding\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    embeddings = [\n",
    "        get_embeddings_from_ollama(modelname, chunk) for chunk in chunks\n",
    "    ]\n",
    "    save_embeddings(filename, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Simpan embedding ke file\n",
    "def save_embeddings(filename, embeddings):\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    with open(f\"embeddings/{filename}.json\", \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "# Cosine similarity untuk menemukan kemiripan\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ac4de-704d-4b68-89aa-61b54123184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Silakan tanya bosku? (ketik 'exit' untuk keluar) ->  hai bot apa kabar?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error or no response received.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Silakan tanya bosku? (ketik 'exit' untuk keluar) ->  apa itu ojk bot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error or no response received.\n"
     ]
    }
   ],
   "source": [
    "# Fungsi utama\n",
    "def main():\n",
    "    SYSTEM_PROMPT = \"\"\"Anda adalah asisten yang membantu menjawab pertanyaan dengan bahasa Indonesia \n",
    "    dan berdasarkan cuplikan teks yang diberikan dalam konteks. Jawab hanya menggunakan konteks yang disediakan, \n",
    "    menjadi sesingkat mungkin. Jika Anda tidak yakin, katakan saja Anda tidak tahu.\n",
    "    Context:\n",
    "    \"\"\"\n",
    "\n",
    "    data_folder = \"data\"\n",
    "    all_paragraphs = []\n",
    "    filenames = []\n",
    "\n",
    "    # Iterasi semua file dalam folder data\n",
    "    for file in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        if file.lower().endswith((\".txt\", \".pdf\", \".docx\")):\n",
    "            paragraphs = parse_file(file_path)\n",
    "            all_paragraphs.extend(paragraphs)\n",
    "            filenames.append(file)\n",
    "\n",
    "    # Buat embedding\n",
    "    embeddings = get_embeddings(\"data_embeddings\", \"nomic-embed-text\", all_paragraphs)\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"Silakan tanya bosku? (ketik 'exit' untuk keluar) -> \")\n",
    "        \n",
    "        if prompt.lower() == \"exit\":\n",
    "            print(\"Exiting the assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Mendapatkan embedding prompt dari Ollama\n",
    "        prompt_embedding = get_embeddings_from_ollama(\"nomic-embed-text\", prompt)\n",
    "\n",
    "        # Temukan paragraf yang paling mirip\n",
    "        most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "\n",
    "        # Persiapkan konteks untuk chat\n",
    "        context = \"\\n\".join(all_paragraphs[item[1]] for item in most_similar_chunks)\n",
    "        \n",
    "        # Kirim request chat ke Ollama\n",
    "        response = chat_with_ollama(\n",
    "            model=\"llama3\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT + context},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            ollama_api_url=OLLAMA_API_URL\n",
    "        )\n",
    "\n",
    "        # Print the full response\n",
    "        if response:\n",
    "            print(\"\\n\\n\")\n",
    "            print(response)  # Output the final model response\n",
    "        else:\n",
    "            print(\"Error or no response received.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
