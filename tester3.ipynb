{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfd7b78-c608-4c2b-9ec4-02bd538f7b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported file format: .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection\n",
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.vectorstores import Milvus\n",
    "\n",
    "# Folder tempat dokumen berada\n",
    "DATA_FOLDER = \"data/\"\n",
    "\n",
    "# Fungsi untuk membaca dokumen\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_name}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Test membaca dokumen\n",
    "docs = load_documents(DATA_FOLDER)\n",
    "\n",
    "# Inisialisasi text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Potong dokumen menjadi chunk\n",
    "chunked_docs = []\n",
    "for doc in docs:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    chunked_docs.extend(chunks)\n",
    "\n",
    "# Inisialisasi model embedding (menggunakan nomic-embed-text)\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Buat embedding untuk setiap chunk dokumen\n",
    "embeddings = [embedding_model.embed_query(chunk) for chunk in chunked_docs]\n",
    "\n",
    "# Koneksi ke Milvus server\n",
    "connections.connect(host=\"192.168.30.222\", port=\"19530\")\n",
    "\n",
    "# Definisi schema untuk koleksi (Milvus)\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768),  # Dimensi embedding sesuai model\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"Embedding storage for chatbot\")\n",
    "\n",
    "# Buat koleksi di Milvus\n",
    "collection_name = \"chatbot_embeddings\"\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# Buat indeks\n",
    "collection.create_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}}\n",
    ")\n",
    "\n",
    "# Muat koleksi dan indeks ke memori\n",
    "collection.load()\n",
    "\n",
    "# Menyisipkan embedding ke Milvus\n",
    "collection.insert([embeddings])\n",
    "\n",
    "# Menyimpan perubahan ke Milvus\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4135b4c-a68f-441e-8504-bfe7ac14bcc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Menyesuaikan dengan setup model kamu\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Setup LangChain untuk RAG\u001b[39;00m\n\u001b[0;32m      5\u001b[0m retrieval_qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m      6\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      7\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# \"stuff\" method, gabungkan pencarian dan generasi\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     retriever\u001b[38;5;241m=\u001b[39m\u001b[43mMilvus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Test retrieval dan generation dengan query\u001b[39;00m\n\u001b[0;32m     12\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the main topic of the document?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chatbot\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:841\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    831\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \n\u001b[0;32m    833\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m     texts \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    842\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\chatbot\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:841\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    831\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \n\u001b[0;32m    833\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    842\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# Setup Llama3 model untuk RAG\n",
    "llm = Ollama(model=\"llama3\")  # Menyesuaikan dengan setup model kamu\n",
    "\n",
    "# Setup LangChain untuk RAG\n",
    "retrieval_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" method, gabungkan pencarian dan generasi\n",
    "    retriever=Milvus.from_documents(chunked_docs, embeddings)\n",
    ")\n",
    "\n",
    "# Test retrieval dan generation dengan query\n",
    "query = \"What is the main topic of the document?\"\n",
    "response = retrieval_qa_chain.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a014bf1-51e9-4d79-aace-54dd230f9f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported file format: .ipynb_checkpoints\n",
      "Loaded 1 documents.\n",
      "nama saya fikri rama saya kerja sebagai data analyst umur saya 27 saya bingung harus apa teman saya ada 5 orang\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "# Folder tempat dokumen berada\n",
    "DATA_FOLDER = \"data/\"\n",
    "\n",
    "# Fungsi untuk membaca dokumen\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if file_name.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_name.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load documents from the file and extend the list\n",
    "        doc_texts = loader.load()  # loader.load() returns a list of documents (probably with text as the attribute)\n",
    "        \n",
    "        # Extract text from documents if the loader returns objects\n",
    "        for doc in doc_texts:\n",
    "            documents.append(doc.page_content)  # Assuming 'page_content' holds the text data for each document\n",
    "            \n",
    "    return documents\n",
    "\n",
    "# Test membaca dokumen\n",
    "docs = load_documents(DATA_FOLDER)\n",
    "print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# Fungsi untuk preprocess dan membersihkan teks\n",
    "def preprocess_text(documents):\n",
    "    # Contoh preprocessing: mengganti newline dengan spasi dan mengubah semua teks ke lowercase\n",
    "    return [doc.replace(\"\\n\", \" \").lower() for doc in documents]\n",
    "\n",
    "# Preprocess dan bersihkan teks\n",
    "cleaned_documents = preprocess_text(docs)\n",
    "\n",
    "# Tampilkan dokumen pertama setelah diproses\n",
    "print(cleaned_documents[0])  # Print first cleaned document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b98929-5185-4065-abd1-8abbcc746275",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.disconnect(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4fdd920-bc86-477c-bc45-88cd585ba5dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Collection' has no attribute 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Create the collection if it doesn't exist\u001b[39;00m\n\u001b[0;32m     33\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collection_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mCollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m():\n\u001b[0;32m     35\u001b[0m     collection \u001b[38;5;241m=\u001b[39m Collection(name\u001b[38;5;241m=\u001b[39mcollection_name, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Collection' has no attribute 'list'"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "from langchain.vectorstores import Milvus\n",
    "from pymilvus import Collection, CollectionSchema, FieldSchema, DataType\n",
    "from pymilvus import connections\n",
    "\n",
    "# Koneksi ke Milvus server\n",
    "connections.connect(host=\"192.168.30.222\", port=\"19530\")\n",
    "\n",
    "# Initialize OllamaEmbeddings (you can use your own model here)\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  # Use your Ollama embedding model here\n",
    "\n",
    "def create_milvus_collection():\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768),  # Adjust dim based on your model\n",
    "        FieldSchema(name=\"document\", dtype=DataType.VARCHAR, max_length=500)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields)\n",
    "    collection = Collection(name=\"abc\", schema=schema)\n",
    "    return collection\n",
    "\n",
    "collection = create_milvus_collection()\n",
    "\n",
    "# Connect to Milvus\n",
    "milvus_host = \"192.168.30.222\"  # Your Milvus server IP\n",
    "milvus_port = \"19530\"  # Default port\n",
    "connections.connect(\"default\", host=milvus_host, port=milvus_port)\n",
    "\n",
    "# Create the collection if it doesn't exist\n",
    "collection_name = \"abc\"\n",
    "if collection_name not in Collection.list():\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    print(f\"Collection '{collection_name}' created.\")\n",
    "else:\n",
    "    collection = Collection(collection_name)\n",
    "    print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "# Convert cleaned documents (strings) into Document objects\n",
    "documents = [Document(page_content=doc) for doc in cleaned_documents]\n",
    "\n",
    "# Embed documents using Ollama embeddings\n",
    "document_embeddings = embeddings.embed_documents([doc.page_content for doc in documents])\n",
    "\n",
    "# Insert documents into Milvus\n",
    "collection.insert([[\"text\"] * len(documents), document_embeddings])\n",
    "\n",
    "# Create an index on the vector field\n",
    "index_params = {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 100}}\n",
    "collection.create_index(field_name=\"vector\", index_params=index_params)\n",
    "\n",
    "# Verify index creation\n",
    "print(\"Index crea\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
